{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bài tập thực hành Nhập môn máy học - Lab 08\n",
    "## MSSV: 18110014 - Họ tên: Nguyễn Phú Thành"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Hãy cài đặt thêm một module để chọn ra được bộ weights sao cho accuracy trên tập validation là tốt nhất"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_vector(y):\n",
    "    out = np.zeros((y.shape[0], y.max() + 1))\n",
    "    for i in range(y.shape[0]):\n",
    "        out[i, y[i]] = 1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"https://raw.githubusercontent.com/huynhthanh98/ML/master/lab-08/bt_train.csv\")\n",
    "valid = pd.read_csv(\"https://raw.githubusercontent.com/huynhthanh98/ML/master/lab-08/bt_valid.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train.iloc[:, :2].to_numpy(), train.iloc[:, -1].to_numpy()\n",
    "X_valid, y_valid = valid.iloc[:, :2].to_numpy(), valid.iloc[:, -1].to_numpy()\n",
    "mean, std = np.mean(X_train, axis = 0), np.std(X_train, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = (X_train - mean)/std\n",
    "y_train = one_hot_vector(y_train)\n",
    "X_valid = (X_valid - mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid = X_train.T, X_valid.T\n",
    "y_train = y_train.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputLayer:\n",
    "    def __init__(self, inputs):\n",
    "        self.inputs = inputs.copy()\n",
    "        self.ac_X = self.inputs\n",
    "\n",
    "class HiddenLayer:\n",
    "    def __init__(self, activation_func, derivative_activation, input_shape, output_shape, weights = None, bias = None, random_state = 0):\n",
    "        self.activation, self.derivative = activation_func, derivative_activation\n",
    "        self.inp, self.outp = input_shape, output_shape\n",
    "        if weights is None:\n",
    "            rnd = np.random.RandomState(random_state)\n",
    "            self.weights = rnd.normal(loc = 0, scale = 1, size = (self.outp, self.inp))\n",
    "        else:\n",
    "            self.weights = weights.copy()\n",
    "        \n",
    "        if bias is None:\n",
    "            rnd = np.random.RandomState(random_state)\n",
    "            self.bias = rnd.normal(loc = 0, scale = 1, size = (self.outp, 1))\n",
    "        else:\n",
    "            self.bias = bias.copy()\n",
    "    \n",
    "    def forward(self, X):\n",
    "        assert X.shape[0] == self.inp, f'{X.shape} and {self.weights.shape} do not match'\n",
    "        self.z_X = self.weights @ X + self.bias\n",
    "        self.ac_X = self.activation(self.z_X)\n",
    "        return self.ac_X\n",
    "    \n",
    "    def update(self, weight_derivative, bias_derivative, lr = 1e-3):\n",
    "        self.weights -= lr * weight_derivative\n",
    "        self.bias -= lr * bias_derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing delta rule based on: http://cs229.stanford.edu/notes-spring2019/backprop.pdf\n",
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, layers):\n",
    "        \n",
    "        assert isinstance(layers[0], InputLayer), 'First layer in layers list must be an instance of InputLayer'\n",
    "        for i, layer in enumerate(layers[1:]):\n",
    "            assert isinstance(layer, HiddenLayer), f'{i + 1}th layer is not an instance of HiddenLayer'\n",
    "        self.layers, self.number_layers = layers, len(layers)\n",
    "    \n",
    "    def forward_fit(self):\n",
    "        return self.forward(self.layers[0].inputs)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        current_X = X\n",
    "        for layer in self.layers[1:]:\n",
    "            current_X = layer.forward(current_X)\n",
    "        outputs = current_X.copy()\n",
    "        return outputs\n",
    "    \n",
    "    def __backpropagation(self, deltas, lr = 1e-3):\n",
    "        \n",
    "        assert len(deltas) == 1 and isinstance(deltas, list)\n",
    "        \n",
    "        for l in range(self.number_layers - 2, 0, -1):\n",
    "            delta_l = (self.layers[l + 1].weights.T @ deltas[-1]) * self.layers[l].derivative(self.layers[l].z_X)\n",
    "            deltas.append(delta_l)\n",
    "        \n",
    "        deltas = [None,] + deltas[::-1]\n",
    "        \n",
    "        for l in range(1, self.number_layers):\n",
    "            weights_gradient = deltas[l] @ self.layers[l - 1].ac_X.T\n",
    "            bias_gradient = np.sum(deltas[l], axis = 1).reshape(-1, 1)\n",
    "            self.layers[l].update(weights_gradient, bias_gradient, lr)\n",
    "    \n",
    "    def backward(self, lr, deltas = None):\n",
    "        self.__backpropagation(deltas, lr = lr)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    return np.where(X >= 0, X, 0).reshape(X.shape)\n",
    "def relu_deriv(X):\n",
    "    return np.where(X >= 0, 1, 0).reshape(X.shape)\n",
    "\n",
    "def softmax(X):\n",
    "    return np.exp(X)/np.sum(np.exp(X), axis = 0)\n",
    "\n",
    "def softmax_deriv(X):\n",
    "    return softmax(X) * (1 - softmax(X))\n",
    "\n",
    "def crossEntropyLoss(y_pred, y):\n",
    "    return -np.mean(y * np.log(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xavier/He Weights Initialization\n",
    "rnd = np.random.RandomState(0)\n",
    "W_1 = rnd.normal(loc = 0, scale = np.sqrt(np.sqrt(2/7)), size = 10).reshape(5, 2)\n",
    "W_2 = rnd.normal(loc = 0, scale = np.sqrt(np.sqrt(1/5)), size = 25).reshape(5, 5)\n",
    "W_3 = rnd.normal(loc = 0, scale = np.sqrt(np.sqrt(1/4)), size = 15).reshape(3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = InputLayer(X_train)\n",
    "\n",
    "hidden_layer_1 = HiddenLayer(\n",
    "    relu, relu_deriv, \n",
    "    input_shape = 2, \n",
    "    output_shape = 5, \n",
    "    weights = W_1,\n",
    "    bias = np.zeros((5, 1))\n",
    ")\n",
    "\n",
    "hidden_layer_2 = HiddenLayer(\n",
    "    relu, relu_deriv, \n",
    "    input_shape = 5, output_shape = 5,\n",
    "    weights = W_2,\n",
    "    bias = np.zeros((5, 1))\n",
    ")\n",
    "\n",
    "outputs_layer = HiddenLayer(\n",
    "    softmax, softmax_deriv, \n",
    "    input_shape = 5, output_shape = 3,\n",
    "    weights = W_3,\n",
    "    bias = np.zeros((3, 1))\n",
    ")\n",
    "\n",
    "layers = [inputs, hidden_layer_1, hidden_layer_2, outputs_layer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(layers)\n",
    "learning_rate = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 1 = 0.9054156080014535\n",
      "Loss at epoch 1001 = 0.09903687116188255\n",
      "Loss at epoch 2001 = 0.09252157060749003\n",
      "Loss at epoch 3001 = 0.0900481598230015\n",
      "Loss at epoch 4001 = 0.08802110390296078\n",
      "Loss at epoch 5001 = 0.08691862834524443\n",
      "Loss at epoch 6001 = 0.08637856676708464\n",
      "Loss at epoch 7001 = 0.08607412398314228\n",
      "Loss at epoch 8001 = 0.08582498628969219\n",
      "Loss at epoch 9001 = 0.08560409543555238\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10000):\n",
    "    y_predict = model.forward_fit()\n",
    "    err = crossEntropyLoss(y_predict, y_train)\n",
    "    deltas = [(y_predict - y_train), ]\n",
    "    model.backward(learning_rate, deltas = deltas)\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        print(f'Loss at epoch {epoch + 1} = {err}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6366666666666667"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation accuracy\n",
    "np.sum(np.argmax(model.forward(X_valid), axis = 0) == y_valid)/y_valid.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.37718074,  0.19974955],\n",
       "       [ 0.75031617,  1.57350461],\n",
       "       [ 1.66724353, -0.67486532],\n",
       "       [ 0.72086112, -0.26184145],\n",
       "       [-1.30794726,  0.17208461]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1].weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1255196 ],\n",
       "       [0.63187546],\n",
       "       [0.02784213],\n",
       "       [0.78581558],\n",
       "       [0.28105539]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1].bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.46761282,  1.12426381,  1.03605959,  0.38676033,  0.03701571],\n",
       "       [-0.09157087,  0.57934273, -0.32959694,  0.26076841, -0.73564545],\n",
       "       [-1.61618643,  0.79291794,  0.51021437, -0.67565562,  1.83027268],\n",
       "       [-1.13362207, -0.37797591, -0.21898696,  1.17646538,  1.23182072],\n",
       "       [ 0.10853918,  0.26951828, -0.59277399, -1.32434061, -0.22336907]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[2].weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.04152722],\n",
       "       [-0.02346769],\n",
       "       [ 0.05036443],\n",
       "       [ 0.63380937],\n",
       "       [ 0.00355501]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[2].bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Từ bộ dữ liệu bên dưới hãy cài đặt backpropagation cho bài toán phân biệt ung thư vú. Hãy tự chọn số layers và số nodes mà mình cho là thích hợp, cũng như là nêu ra số layers và số nodes của mỗi layer mà mình đã chọn. Tính accuracy trên tập training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "breast_cancer = datasets.load_breast_cancer()\n",
    "X = breast_cancer.data\n",
    "y = breast_cancer.target\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mean = np.mean(X_train, axis = 0).reshape(1, -1)\n",
    "X_std = np.std(X_train, axis = 0).reshape(1, -1)\n",
    "\n",
    "X_valid = (X_valid - X_mean)/X_std\n",
    "X_train = (X_train - X_mean)/X_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid = X_train.T, X_valid.T\n",
    "y_train = one_hot_vector(y_train).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>\n",
    "        <h3> Số lượng layers: 5 (bao gồm Input layer) </h3>\n",
    "        <ol>\n",
    "            <li>\n",
    "                Layer 1: Input layer\n",
    "                <ul>\n",
    "                    <li> Số node: 30 </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>\n",
    "                Layer 2: Hiddent layer 1\n",
    "                <ul>\n",
    "                    <li> Số node: 20 </li>\n",
    "                    <li> Activation function: Linear </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>\n",
    "                Layer 3: Hidden layer 2\n",
    "                <ul>\n",
    "                    <li> Số node: 20 </li>\n",
    "                    <li> Activation function: Softmax </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>\n",
    "                Layer 4: Hidden layer 3\n",
    "                <ul>\n",
    "                    <li> Số node: 10 </li>\n",
    "                    <li> Activation function: Linear </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>\n",
    "                Layer 5: Output layer\n",
    "                <ul>\n",
    "                    <li> Số node: 2 </li>\n",
    "                    <li> Activation function: Softmax </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ol>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearFunc(X):\n",
    "    return X.copy()\n",
    "\n",
    "def linearDeriv(X):\n",
    "    return np.ones(X.shape, dtype = X.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xavier Weights Initialization for linear/sigmoid activation function\n",
    "rnd = np.random.RandomState(0)\n",
    "W_1 = rnd.uniform(low = -1/np.sqrt(30), high = 1/np.sqrt(30), size = 600).reshape(20, 30)\n",
    "W_2 = rnd.uniform(low = -1/np.sqrt(20), high = 1/np.sqrt(20), size = 400).reshape(20, 20)\n",
    "W_3 = rnd.uniform(low = -1/np.sqrt(20), high = 1/np.sqrt(20), size = 200).reshape(10, 20)\n",
    "W_4 = rnd.uniform(low = -1/np.sqrt(10), high = 1/np.sqrt(10), size = 20).reshape(2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = InputLayer(X_train)\n",
    "\n",
    "hidden_layer_1 = HiddenLayer(\n",
    "    linearFunc, linearDeriv, \n",
    "    input_shape = 30, output_shape = 20, \n",
    "    weights = W_1,\n",
    "    bias = np.zeros((20, 1))\n",
    ")\n",
    "\n",
    "hidden_layer_2 = HiddenLayer(\n",
    "    softmax, softmax_deriv, \n",
    "    input_shape = 20, output_shape = 20,\n",
    "    weights = W_2,\n",
    "    bias = np.zeros((20, 1))\n",
    ")\n",
    "\n",
    "hidden_layer_3 = HiddenLayer(\n",
    "    linearFunc, linearDeriv,\n",
    "    input_shape = 20, output_shape = 10,\n",
    "    weights = W_3,\n",
    "    bias = np.zeros((10, 1))\n",
    ")\n",
    "\n",
    "outputs_layer = HiddenLayer(\n",
    "    softmax, softmax_deriv, \n",
    "    input_shape = 10, output_shape = 2,\n",
    "    weights = W_4,\n",
    "    bias = np.zeros((2, 1))\n",
    ")\n",
    "\n",
    "layers = [\n",
    "    inputs, \n",
    "    hidden_layer_1, \n",
    "    hidden_layer_2, \n",
    "    hidden_layer_3, \n",
    "    outputs_layer\n",
    "]\n",
    "\n",
    "model = NeuralNetwork(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 1 = 0.34485360860141645\n",
      "Loss at epoch 1001 = 0.32666028467515945\n",
      "Loss at epoch 2001 = 0.32112527887156944\n",
      "Loss at epoch 3001 = 0.3006582985820205\n",
      "Loss at epoch 4001 = 0.17096822138709328\n",
      "Loss at epoch 5001 = 0.07947332241577691\n",
      "Loss at epoch 6001 = 0.05378556595997626\n",
      "Loss at epoch 7001 = 0.0424174313694559\n",
      "Loss at epoch 8001 = 0.03649459736942651\n",
      "Loss at epoch 9001 = 0.033091324766509216\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-5\n",
    "for epoch in range(10000):\n",
    "    y_predict = model.forward_fit()\n",
    "    err = crossEntropyLoss(y_predict, y_train)\n",
    "    deltas = [y_predict - y_train, ]\n",
    "    model.backward(learning_rate, deltas = deltas)\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        print(f'Loss at epoch {epoch + 1} = {err}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9802197802197802"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training accuracy\n",
    "np.sum(np.argmax(model.forward(X_train), axis = 0) == np.argmax(y_train, axis = 0))/y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9912280701754386"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing accuracy\n",
    "np.sum(np.argmax(model.forward(X_valid), axis = 0) == y_valid)/y_valid.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,\n",
       "       1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
       "       0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,\n",
       "       0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,\n",
       "       0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n",
       "       1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,\n",
       "       1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,\n",
       "       0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,\n",
       "       1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,\n",
       "       1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(model.forward(X_train), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,\n",
       "       1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
       "       0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n",
       "       1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,\n",
       "       0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n",
       "       1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,\n",
       "       1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,\n",
       "       0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,\n",
       "       1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(y_train, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,\n",
       "       0, 1, 1, 0])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(model.forward(X_valid), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,\n",
       "       0, 1, 1, 0])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
